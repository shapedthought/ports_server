{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21212193",
   "metadata": {},
   "source": [
    "## Updated Scrape ports\n",
    "\n",
    "The updated scrape ports has some new efficiencies.\n",
    "\n",
    "1. It reads the URLs from a config file instead of hardcoding them in the script.\n",
    "2. It uses a loop to iterate through the URLs, making it easier to add or remove URLs in the future.\n",
    "3. It iterates through the HTML looking for Subheadings which it tracks then when a table is found it creates a dataframe of which the subheadings are added. This provides a more structured output which aligns with how the data is presented on the website.\n",
    "4. It adds up to three levels of subheadings to the dataframe. This allows for a more detailed representation of the data.\n",
    "5. Iterating through the HTML like this means that there are less unwanted artifacts in the data.\n",
    "6. Doing all of this means that this code is less fragile and can be used to automate the updating of the data regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d31ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "with open('configuration.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87ce4b",
   "metadata": {},
   "source": [
    "Above reads all the configuration data from the configuration file as well as loading all the required libraries.\n",
    "\n",
    "Below is the code that does the scraping. \n",
    "\n",
    "1. It iterates through the URLs in the config file and gets the HTML.\n",
    "2. It uses BeautifulSoup to parse the HTML and find all the subheadings and tables.\n",
    "3. It tracks the subheadings Subheading, Subheading_L2, and Subheading_L3.\n",
    "4. When it finds a table, it creates a DataFrame and adds the subheadings to the DataFrame.\n",
    "5. It appends the DataFrame to a list of DataFrames for that URL.\n",
    "6. When all the tables for a URL have been processed, it concatenates the DataFrames and saves them to a central list.\n",
    "7. Finally, it concatenates all the DataFrames from all URLs for a final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store all intermediate DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Iterate over each item in the config array\n",
    "for entry in config:\n",
    "    product = entry['product']\n",
    "    url = entry['url']\n",
    "    \n",
    "    try:\n",
    "        # Fetch the HTML content\n",
    "        html = requests.get(url)\n",
    "        soup = bs(html.content, \"html.parser\")\n",
    "\n",
    "        # Initialize variables to track headings\n",
    "        current_subheading = \"\"\n",
    "        current_subheading_l2 = \"\"\n",
    "        current_subheading_l3 = \"\"\n",
    "\n",
    "        # List to store DataFrames for this product\n",
    "        product_dataframes = []\n",
    "\n",
    "        # Find all elements in the body\n",
    "        all_elements = soup.body.find_all(['span', 'table'])\n",
    "\n",
    "        # Iterate through the elements\n",
    "        for element in all_elements:\n",
    "            if element.name == 'span' and 'class' in element.attrs:\n",
    "                element_classes = element.get('class', [])\n",
    "                if 'Subheading' in element_classes:\n",
    "                    current_subheading = element.get_text(strip=True)\n",
    "                    current_subheading_l2 = \"\"\n",
    "                    current_subheading_l3 = \"\"\n",
    "                elif 'Subheading_L2' in element_classes:\n",
    "                    current_subheading_l2 = element.get_text(strip=True)\n",
    "                    current_subheading_l3 = \"\"\n",
    "                elif 'Subheading_L3' in element_classes:\n",
    "                    current_subheading_l3 = element.get_text(strip=True)\n",
    "            elif element.name == 'table':\n",
    "                # Convert the table to a DataFrame\n",
    "                df = pd.read_html(io.StringIO(str(element)))[0]\n",
    "\n",
    "                # Add the headings as columns\n",
    "                df['Subheading'] = current_subheading\n",
    "                df['Subheading_L2'] = current_subheading_l2\n",
    "                df['Subheading_L3'] = current_subheading_l3\n",
    "\n",
    "                # Add the product column\n",
    "                df['Product'] = product\n",
    "\n",
    "                # Append the DataFrame to the product-specific list\n",
    "                product_dataframes.append(df)\n",
    "\n",
    "        # Combine all DataFrames for this product\n",
    "        if product_dataframes:\n",
    "            combined_product_df = pd.concat(product_dataframes, ignore_index=True)\n",
    "            all_dataframes.append(combined_product_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {product} ({url}): {e}\")\n",
    "\n",
    "# Combine all intermediate DataFrames into a single DataFrame\n",
    "final_combined_df = pd.concat(all_dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6aa28",
   "metadata": {},
   "source": [
    "The follow code is design to clean up the data and remove any unneeded information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76be4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df[\"Description\"] = np.where(\n",
    "    final_combined_df[\"Notes\"].notna(), final_combined_df[\"Notes\"], final_combined_df[\"Description\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df['Description'] = final_combined_df['Description'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df['Subheading'] = np.where(final_combined_df['Subheading'] == '', final_combined_df['From'], final_combined_df['Subheading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df['Port'] = np.where(\n",
    "    (final_combined_df['Port'].isna()) & (~final_combined_df['Port/Endpoint'].isna()),\n",
    "    final_combined_df['Port/Endpoint'],\n",
    "    final_combined_df['Port']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [0, 'Port/Endpoint', 'Notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef271b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df.drop(columns=columns_to_drop, inplace=True, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df = final_combined_df.dropna(subset=['Port'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [\"Other Communications\",  \"Communication with Backup Server\", \"Communication with Backup Infrastructure Components\", \"Depends on device configuration\", \"Communication with Virtualization Servers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da84984",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df['Port'] = final_combined_df['Port'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df = pd.read_parquet('final_combined_df.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_combined_df[final_combined_df['Subheading_L2'] == 'IBM FlashSystem (formerly Spectrum Virtualize) Storage']\n",
    "\n",
    "final_combined_df['Subheading_L2'] = np.where((final_combined_df['Subheading_L2'] == 'IBM FlashSystem (formerly Spectrum Virtualize) Storage') & (final_combined_df['Subheading_L3'] != \"\" ), final_combined_df['Subheading_L3'], final_combined_df['Subheading_L2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df[(final_combined_df['Subheading_L2'] == final_combined_df['Subheading_L3']) & (final_combined_df['Subheading_L2'] != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1140d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df['Subheading_L3'] = np.where(\n",
    "    (final_combined_df['Subheading_L2'] == final_combined_df['Subheading_L3']) & \n",
    "    (final_combined_df['Subheading_L2'] != \"\") & \n",
    "    (final_combined_df['Subheading_L3'] != \"\"), \n",
    "    \"\", \n",
    "    final_combined_df['Subheading_L3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f431c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df = final_combined_df[~final_combined_df['Port'].isin(rows_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df.loc[:, 'Port'] = final_combined_df['Port'].str.split('(').str[0].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418300a1",
   "metadata": {},
   "source": [
    "This section creates the sqlite database, the all_ports table and then inserts the data into the table.\n",
    "\n",
    "This has been updated since the first version as it now uses: subheading, subheading_l2 and subheading_l3 to create a more structured table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the SQLite connection is established\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "con = sqlite3.connect(\"allports_updated.db\")\n",
    "cur = con.cursor()\n",
    "\n",
    "# Create the table if it doesn't already exist\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS all_ports (\n",
    "        product TEXT,\n",
    "        subheading TEXT,\n",
    "        subheading_l2 TEXT,\n",
    "        subheading_l3 TEXT,\n",
    "        from_port TEXT,\n",
    "        to_port TEXT,\n",
    "        protocol TEXT,\n",
    "        port TEXT,\n",
    "        description TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Insert all rows from final_combined_df into the database\n",
    "columns = ['Product', 'Subheading', 'Subheading_L2', 'Subheading_L3', 'From', 'To', 'Protocol', 'Port', 'Description']\n",
    "data = final_combined_df[columns].values.tolist()\n",
    "\n",
    "cur.executemany(\n",
    "    \"\"\"\n",
    "    INSERT INTO all_ports (product, subheading, subheading_l2, subheading_l3, from_port, to_port, protocol, port, description)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\",\n",
    "    data\n",
    ")\n",
    "\n",
    "# Commit the transaction and close the connection\n",
    "con.commit()\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
